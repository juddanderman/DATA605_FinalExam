---
title: "Data 605 Final Exam"
author: "Judd Anderman"
date: "May 24, 2017"
output: html_document
---

```{r setup, echo = FALSE}
## set working directory as needed
setwd("~/DATA 605 Fundamentals of Computational Math/DATA605_FinalExam")
```

```{r import}
train <- read.csv("train.csv", header = TRUE)
```

### Probability

Let X = `GrLivArea`  
x = $Q_4$ (4th quartile) of X  
Y = `SalePrice`  
y = $Q_2$ (2nd quartile) of Y

```{r vars-quartiles}
df <- data.frame(X = train$GrLivArea, Y = train$SalePrice)
quantile(df$X, c(0, 0.25, 0.5, 0.75, 1))
quantile(df$Y, c(0, 0.25, 0.5, 0.75, 1))
```

```{r vars-prob}
pdf <- function(var) {
  approxfun(density(var))
}
cdf <- function(samp, val) {
  return(integrate(pdf(samp), min(samp), min(val, max(samp)))[1]$value)
}

hist(df$X, probability = TRUE, 
     ylim = c(0, max(density(df$X)$y)))
lines(density(df$X))
plot(ecdf(df$X))
a <- seq(min(df$X), max(df$X), (max(df$X) - min(df$X)) / 100)
plot(a, sapply(a, function(z) cdf(df$X, z)), type = "l")

hist(df$Y, probability = TRUE,
     ylim = c(0 , max(density(df$Y)$y)))
lines(density(df$Y))
plot(ecdf(df$Y))
b <- seq(min(df$Y), max(df$Y), (max(df$Y) - min(df$Y)) / 100)
plot(b, sapply(b, function(z) cdf(df$Y, z)), type = "l")

(pr_A <- (nrow(df[df$X > max(df$X) & df$Y > median(df$Y), ]) / nrow(df)) / 
  (nrow(df[df$Y > median(df$Y), ]) / nrow(df)))

(pr_B <- nrow(df[df$X > max(df$X) & df$Y > median(df$Y), ]) / nrow(df))

(pr_C <- (nrow(df[df$X < max(df$X) & df$Y > median(df$Y), ]) / nrow(df)) /
  (nrow(df[df$Y > median(df$Y), ]) / nrow(df)))
```

a\. $P(X > x | Y > y) = P(X > x \cap Y > y) / P(Y > y) = P(X > 5642 \cap Y > 163000) / P(Y > 163000) = (0 / 1460) / (728 / 1460) = `r pr_A`$

This is the probability that X or `GrLivArea`, the above grade (ground) living area in square feet, is greater than the fourth quartile or 100th percentile of that variable conditioned on the event that Y or `SalePrice`, the property's sale price in dollars, is greater than the second quartile or median value of that variable.

b\. $P(X > x, Y > y) = P(X > 5642 \cap Y > 163000) = 0 / 1460 = `r pr_B`$

This is the joint probability that a property's `GrLivArea` is greater than the fourth quartile of that variable and its `SalePrice` is greater than the second quartile of that variable.  

c\. $P(X < x | Y > y) = P(X < x \cap Y > y) / P(Y > y) = P(X < 5642 \cap Y > 163000) / P(Y > 163000) = (728 / 1460) / (728 / 1460) = `r pr_C`$

This is the conditional probability that `GrLivArea` is less than the fourth quartile of that variable given that `SalePrice` is greater than the second quartile of that variable.

```{r independence}
(cond_pr1 <- (nrow(df[df$X > max(df$X) & df$Y > median(df$Y), ]) / nrow(df)) /
  (nrow(df[df$Y > median(df$Y), ]) / nrow(df)))

(indep_pr1 <- (nrow(df[df$X > max(df$X), ]) / nrow(df)))

cond_pr1 == indep_pr1

(cond_pr2 <- 
  (nrow(df[df$X > quantile(df$X, 0.75) & df$Y > median(df$Y), ]) / nrow(df)) / 
  (nrow(df[df$Y > median(df$Y), ]) / nrow(df)))

(indep_pr2 <- (nrow(df[df$X > quantile(df$X, 0.75), ]) / nrow(df)))

cond_pr2 == indep_pr2

(cond_pr3 <- (nrow(df[df$X > median(df$X) & df$Y > median(df$Y), ]) / nrow(df)) /
  (nrow(df[df$Y > median(df$Y), ]) / nrow(df)))

(indep_pr3 <- (nrow(df[df$X > median(df$X), ]) / nrow(df)))

cond_pr3 == indep_pr3
```

```{r chisq}
(t1 <- table(df$X > max(df$X), df$Y > median(df$Y)))
chisq.test(t1)

(t2 <- table(df$X > quantile(df$X, 0.75), df$Y > median(df$Y)))
chisq.test(t2)

(t3 <- table(df$X > median(df$X), df$Y > median(df$Y)))
chisq.test(t3)

(t4 <- table(ceiling((ecdf(df$X)(df$X) / 0.25)), ceiling((ecdf(df$Y)(df$Y) / 0.25))))
chisq.test(t4)
```

Above, I test the independence of the variables $X$ and $Y$ by comparing the conditional probability $P(X > x | Y > y)$ with the probability $P(X > x)$ for three values of $x$, $x = {4_Q(X), 3_Q(X), 2_Q(X)}$.  In other words, I compare the conditional probabilities that `GrLivArea` is greater than the fourth quartile, third quartile, and median values for that variable given that `SalePrice` is greater than the median property sale price with the corresponding unconditioned probability of the event that `GrLivArea` is greater than the specified threshold values.  If the two variables were independent, the conditional probability $P(X > x | Y > y)$ would be equal to $P(X > x)$, as the event that $Y > y$ would provide no additional information about the likelihood of $X$ exceeding one of the examined threshold values.  Here, the conditional and unconditioned probabilities are only equal in the case where $x = 4_Q(X)$ since there are no values in $X$ greater than the fourth quartile and so both probabilities are equal to zero.  Since the conditional and unconditioned probabilities found for the other values of $x$ were not equal, we can conclude that the variables $X$ and $Y$ are not independent of one another.  

Chi-squared testing on two-way contingency tables of $X > x$ and $Y > y$ for the threshold values of $x$ used in the comparisons above, as well as on the contingency table comprised of the counts obtained by binning each variable at their respective quartile boundaries, confirm an association between the two variables.  All chi-squared tests aside from the first on `t1` where $x = 4_Q(X)$, which comprises the counts of cases in which $Y > y \cap X < x$ and $Y \leq \cap X < x$ since there are no cases in which $X > x$, yield p-values less than 0.05, so we can reject the null hypothesis that the two variables are independent.

### Descriptive and Inferential Statistics

```{r descriptive}
summary(df$X)
var(df$X)
sd(df$X)
hist(df$X)
boxplot(df$X)

summary(df$Y)
var(df$Y)
sd(df$Y)
hist(df$Y)
boxplot(df$Y)
```

```{r scatter}
plot(df)
qqnorm(lm(Y ~ X, df)$residuals)
qqline(lm(Y ~ X, df)$residuals)
```

```{r box-cox-transform}
library(MASS)

## transform X
bc <- boxcox(X ~ 1, data = df, lambda = seq(-2, 2, len = 1000))
## 95% CI for lambda
range(bc$x[bc$y > max(bc$y) - 1/2 * qchisq(0.95,1)])

lambda_X <- bc$x[which.max(bc$y)]
df$X_bc <- (df$X^lambda_X - 1) / lambda_X

## transform Y
bc <- boxcox(Y ~ 1, data = df, lambda = seq(-2, 2, len = 1000))
## 95% CI for lambda
range(bc$x[bc$y > max(bc$y) - 1/2 * qchisq(0.95,1)])

lambda_Y <- bc$x[which.max(bc$y)]
df$Y_bc <- (df$Y^lambda_Y - 1) / lambda_Y

plot(df$X_bc, df$Y_bc)
qqnorm(lm(Y_bc ~ X_bc, df)$residuals)
qqline(lm(Y_bc ~ X_bc, df)$residuals)

plot(log(df$X), log(df$Y))
qqnorm(lm(log(Y) ~ log(X), df)$residuals)
qqline(lm(log(Y) ~ log(X), df)$residuals)
```

```{r correlation}
library(psychometric)
(r_bc <- cor(df$X_bc, df$Y_bc))
z_r_bc <- 0.5 * log((1 + r_bc)/(1 - r_bc))
se_r <- 1 / sqrt(nrow(df) - 3)
(CIr_bc <- data.frame(lower = (exp(2 * (z_r_bc - qnorm(0.995) * se_r)) - 1) / 
                        (exp(2 * (z_r_bc - qnorm(0.995) * se_r)) + 1),
                      upper = (exp(2 * (z_r_bc + qnorm(0.995) * se_r)) - 1) / 
                        (exp(2 * (z_r_bc + qnorm(0.995) * se_r)) + 1)
                      ))
CIr(r_bc, nrow(df), level = 0.99)

(r_ln <- cor(log(df$X), log(df$Y)))
z_r_ln <- 0.5 * log((1 + r_ln)/(1 - r_ln))
se_r <- 1 / sqrt(nrow(df) - 3)
(CIr_ln <- data.frame(lower = (exp(2 * (z_r_ln - qnorm(0.995) * se_r)) - 1) / 
                        (exp(2 * (z_r_ln - qnorm(0.995) * se_r)) + 1),
                      upper = (exp(2 * (z_r_ln + qnorm(0.995) * se_r)) - 1) / 
                        (exp(2 * (z_r_ln + qnorm(0.995) * se_r)) + 1)
                      ))
CIr(r_ln, nrow(df), level = 0.99)

## permuatation test on Box-Cox transformed variables
cor_coefs <- vector("numeric", 10000)
for (i in 1:10000) {
  Y_prime <- sample(df$Y_bc, length(df$Y_bc), replace = FALSE)
  cor_coefs[i] <- cor(df$X_bc, Y_prime)
}

head(sort(round(cor_coefs, digits = 3), decreasing = TRUE))

(p_val <- sum(abs(cor_coefs) > abs(r_bc)) / length(cor_coefs)) 

## 99% CI - bootstrap method on Box-Cox transformed variables
cor_coefs <- vector("numeric", 10000)
for (i in 1:10000) {
  rows <- sample(1:nrow(df), nrow(df), replace = TRUE)
  cor_coefs[i] <- cor(df[rows, ]$X_bc, df[rows, ]$Y_bc)
}
quantile(cor_coefs, c(0.005, 0.995))

## permuatation test on log transformed variables
cor_coefs <- vector("numeric", 10000)
for (i in 1:10000) {
  Y_prime <- sample(log(df$Y), length(log(df$Y)), replace = FALSE)
  cor_coefs[i] <- cor(log(df$X), Y_prime)
}

head(sort(round(cor_coefs, digits = 3), decreasing = TRUE))

(p_val <- sum(abs(cor_coefs) > abs(r_bc)) / length(cor_coefs)) 

## 99% CI - bootstrap method on log transformed variables
cor_coefs <- vector("numeric", 10000)
for (i in 1:10000) {
  rows <- sample(1:nrow(df), nrow(df), replace = TRUE)
  cor_coefs[i] <- cor(log(df[rows, ]$X), log(df[rows, ]$Y))
}
quantile(cor_coefs, c(0.005, 0.995))
```

After performing Box-Cox transformations on both $X$ and $Y$ using the values of the parameter $\lambda$ with the maximum log-likelihood - and also performing simple log transformations on both variables since the 95% confidence intervals of the log-likelihood optimizing values of $\lambda$ for each straddled zero - I computed the correlation and associated 99% confidence interval for each pair of transformed variables.  Then, I tested the null hypothesis that the true correlation coefficient $\rho$ is equal to zero against the alternative hypothesis that $\rho$ is not equal to zero using a permutation test.  Here, new sets of paired values $(x_i, y_{i'})$ were derived from the original set of paired values $(x_i, y_i)$ by randomly sampling $y_{i'}$ without replacement from all of the values in $y_i$, and the correlation of the permuted value pairs was calculated.  This process was repeated 10,000 times and then a p-value for a two-sided test of the null hypothesis $\rho = 0$ was calculated as the proportion of correlation coefficients in the 10,000 sets of permuted value pairs greater than the value of the correlation coefficient obtained from the original dataset.  In this case, the p-value was equal to zero.  I also applied the bootstrap method to approximate a sampling distribution for $\rho$ and compute a 99% confidence interval.  Here, I performed resampling with replacement of the same number of paired values as contained in the original dataset and then calculated the correlation coefficient of the resampled data.  This process was also iterated 10,000 times and the resulting distribution of resampled correlation coefficients was used as an approximation of the sampling distribution for $\rho$.  The lower boundary of the 99% confidence interval was approximately 0.69, supporting the conclusion of the permutation test.  In addition, the 99% confidence interval obtained through bootstrap sampling agreed closely with the confidence interval estimated earlier using the Fisher transformation.  Very similar results were obtained for hypothesis testing of the correlation coefficient of both the Box-Cox transformed and log-transformed variable pairs, in other words, each pair of transformed variables provided strong evidence against the null hypotheses that the true correlation coefficients are zero.  

\pagebreak

### Linear Algebra and Correlation

```{r invert-cor-mat}
(cor_mat <- cor(data.frame(X_bc = df$X_bc, Y_bc = df$Y_bc)))
(cor_inv <- solve(cor_mat))

cor_mat %*% cor_inv
cor_inv %*% cor_mat

cor_mat %*% cor_inv == cor_inv %*% cor_mat
```

### Calculus-Based Probability and Statistics

```{r fit-dist}
min(df$X) > 0
(nrml_fit <- fitdistr(df$X, densfun = "normal"))
qqnorm(df$X)
qqline(df$X)

h <- hist(df$X)
rnd <- rnorm(1000, mean = nrml_fit$estimate[1], 
             sd = nrml_fit$estimate[2])
par(mfrow = c(1, 2))
plot(h)
hist(rnd,
     main = paste0("Histogram of 1000 samples", "\n", 
                   "from fitted normal", "\n", 
                   " density function"),
     xlab = paste0("Random samples from", "\n", "N(", 
                   round(nrml_fit$estimate[1], digits = 2), ", ",
                   round(nrml_fit$estimate[2], digits = 2), ")"),
     xlim = c(min(c(h$breaks, min(rnd))), max(h$breaks)))
par(mfrow = c(1, 1))

(lognrml_fit <- fitdistr(df$X, densfun = "log-normal"))
qqnorm(log(df$X))
qqline(log(df$X))
rnd <- exp(rnorm(1000, mean = lognrml_fit$estimate[1], 
                 sd = lognrml_fit$estimate[2]))
par(mfrow = c(1, 2))
plot(h)
hist(rnd,
     main = paste0("Histogram of 1000 samples", "\n", 
                   "from fitted log-normal", "\n",
                   "density function"),
     xlab = paste0("Random samples from", "\n", "exp(N(", 
                   round(lognrml_fit$estimate[1], digits = 2), ", ",
                   round(lognrml_fit$estimate[2], digits = 2), "))"),
     xlim = c(min(c(h$breaks, min(rnd))), max(h$breaks)))
par(mfrow = c(1, 1))
```

Using the `fitdistr` function from the _MASS_ package, I fit both normal and, informed by the work above, log-normal density functions to the independent variable $X$.  Comparison of histograms of the original, non-transformed variable and of 1000 samples generated from each of the fitted density functions indicate that while both of the fitted density functions provide good approximations of the center of the distribution of the original variable, the log-normal fit does a much better job of capturing and reflecting the positive or right skew of the original data.  

### Modeling

#### Exploratory analysis & visualization

```{r explore-1, fig.keep = 'all', fig.align = 'left', out.width = '100%'}
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)

str(train)
sapply(train, summary)
sapply(test, summary)

## count missing values in each variable in `train` and `test`
colSums(sapply(train, is.na))[colSums(sapply(train, is.na)) > 0]
colSums(sapply(test, is.na))[colSums(sapply(test, is.na)) > 0]

## check for duplicates
nrow(train) - nrow(unique(train))
nrow(test) - nrow(unique(test))

par(mfrow = c(2, 4))
for(row in 1:10) {
  for (i in 1:4) {
    j <- (row - 1) * 8 + i + 1
    if (j == 80) {break}
    if (is.numeric(train[[j]]) & 
        length(unique(train[[j]])) >= 12) {
      plot(density(train[[j]], na.rm = TRUE),
           main = colnames(train)[j])
    } else {
      barplot(prop.table(table(train[[j]])), 
              main = colnames(train)[j])
    }
  }
}
par(mfrow = c(2, 4))
for(row in 1:10) {
  for (i in 5:8) {
    j <- (row - 1) * 8 + i + 1
    if (j == 80) {break}
    if (is.numeric(train[[j]]) & 
        length(unique(train[[j]])) >= 12) {
      plot(density(train[[j]], na.rm = TRUE),
           main = colnames(train)[j])
    } else {
      barplot(prop.table(table(train[[j]])), 
              main = colnames(train)[j])
    }
  }
}
```

```{r explore-2, fig.keep = 'all', fig.align = 'left', out.width = '100%'}
par(mfrow = c(2, 4))
for(row in 1:10) {
  for (i in 1:8) {
    j <- (row - 1) * 8 + i + 1
    if (j == 80) {break}
    plot(train[[j]], train$SalePrice, main = colnames(train)[j])
  }
}
```

```{r corrplot}
par(mfrow = c(1, 1))
library(corrplot)
cors <- 
  cor(train[sapply(train, is.numeric) & 
              sapply(train, 
                     function(x) length(unique(x)) >= 5)][, -1],
      use = "na.or.complete")
corrplot(cors, method = "square")

cors[, 31]
```

#### Data cleaning & pre-processing

```{r recoding-feature-sel}
kv_class <- 
  data.frame(key = c(20, 30, 40, 45, 50, 60, 70, 75,
               80, 85, 90, 120, 150, 160, 180, 190),
             value = c("1StoryNew", "1StoryOld", 
               "1StoryAttic", "1.5StoryUnf",
               "1.5StoryFin", "2StoryNew",
               "2StoryOld", "2.5Story",
               "SplitLevel", "SplitFoyer",
               "Duplex", "1StoryPUD",
               "1.5StoryPUD", "2StoryPUD",
               "MultiLevelPUD", "TwoFamConvert")
  )

replace_missing <- function(dataset) {
  df <- dataset
  i <- sapply(df, is.factor)
  df[i] <- lapply(df[i], as.character)
  df$MSSubClass <- 
    sapply(df$MSSubClass, 
           function(x) kv_class[kv_class$key == x, ]$value)
  df$MSZoning[is.na(df$MSZoning)] <- "RL"
  df$LotFrontage[is.na(df$LotFrontage)] <- median(df$LotFrontage, na.rm = TRUE)
  df$Alley[is.na(df$Alley)] <- "None"
  df$Utilities[is.na(df$Utilities)] <- "AllPub"
  df$Exterior1st[is.na(df$Exterior1st)] <- "VinylSd"
  df$Exterior2nd[is.na(df$Exterior2nd)] <- "VinylSd"
  df$MasVnrType[is.na(df$MasVnrType)] <- "None"
  df$MasVnrArea[is.na(df$MasVnrArea)] <- 0
  df$BsmtQual[is.na(df$BsmtQual)] <- "None"
  df$BsmtCond[is.na(df$BsmtCond)] <- "None"
  df$BsmtExposure[is.na(df$BsmtExposure)] <- "None"
  df$BsmtFinType1[is.na(df$BsmtFinType1)] <- "None"
  df$BsmtFinSF1[is.na(df$BsmtFinSF1)] <- 0
  df$BsmtFinType2[is.na(df$BsmtFinType2)] <- "None"
  df$BsmtFinSF2[is.na(df$BsmtFinSF2)] <- 0
  df$BsmtUnfSF[is.na(df$BsmtUnfSF)] <- 0
  df$TotalBsmtSF[is.na(df$TotalBsmtSF)] <- 0
  df$Electrical[is.na(df$Electrical)] <- "SBrkr"
  df$BsmtFullBath[is.na(df$BsmtFullBath)] <- 0
  df$BsmtHalfBath[is.na(df$BsmtHalfBath)] <- 0
  df$KitchenQual[is.na(df$KitchenQual)] <- "TA"
  df$Functional[is.na(df$Functional)] <- "Typ"
  df$FireplaceQu[is.na(df$FireplaceQu)] <- "None"
  df$GarageType[is.na(df$GarageType)] <- "None"
  df$GarageYrBlt[is.na(df$GarageYrBlt)] <- min(df$GarageYrBlt, na.rm = TRUE)
  df$GarageFinish[is.na(df$GarageFinish)] <- "None"
  df$GarageCars[is.na(df$GarageCars)] <- 0
  df$GarageArea[is.na(df$GarageArea)] <- 0
  df$GarageQual[is.na(df$GarageQual)] <- "None"
  df$GarageCond[is.na(df$GarageCond)] <- "None"
  df$PoolQC[is.na(df$PoolQC)] <- "None"
  df$Fence[is.na(df$Fence)] <- "None"
  df$MiscFeature[is.na(df$MiscFeature)] <- "None"
  df$SaleType[is.na(df$SaleType)] <- "WD"
  i <- sapply(df, is.character)
  df[i] <- lapply(df[i], as.factor)
  return(df)
}

kv_bldg_type <- 
  data.frame(key = c("2fmCon", "Duplex", "Twnhs", "TwnhsE", "1Fam"),
             value = 1:5
  )

kv_ext_qual <- 
  data.frame(key = c("Po", "Fa", "TA", "Gd", "Ex"),
             value = 1:5)

kv_ext_cond <- 
  data.frame(key = c("Po", "Fa", "TA", "Gd", "Ex"),
             value = 1:5)

kv_bsmt_qual <- 
  data.frame(key = c("None", "Po", "Fa", "TA", "Gd", "Ex"),
             value = 0:5)

kv_bsmt_cond <- 
  data.frame(key = c("Po", "None", "Fa", "TA", "Gd", "Ex"),
             value = 0:5)

kv_bsmt_exp <- 
  data.frame(key = c("None", "No", "Mn", "Av", "Gd"),
             value = 0:4)

kv_heat_qc <- 
  data.frame(key = c("Po", "Fa", "TA", "Gd", "Ex"),
             value = 1:5)

kv_electrical <-
  data.frame(key = c("Mix", "FuseP", "FuseF", "FuseA", "SBrkr"),
             value = 1:5)

kv_kitchen <- 
  data.frame(key = c("Po", "Fa", "TA", "Gd", "Ex"),
             value = 1:5)

kv_fireplace_q <- 
  data.frame(key = c("Po", "None", "Fa", "TA", "Gd", "Ex"),
             value = 0:5)

kv_garage_fin <- 
  data.frame(key = c("None", "Unf", "RFn", "Fin"),
             value = 0:3)

kv_paved_drive <- 
  data.frame(key = c("N", "P", "Y"), value = 1:3)

recode <- function(dataset) {
  # categorical
  df <- dataset
  i <- sapply(df, is.factor)
  df[i] <- lapply(df[i], as.character)
  df$BldgType <- 
    sapply(df$BldgType, 
           function(x) kv_bldg_type[kv_bldg_type$key == x, ]$value)
  df$ExterQual <- 
    sapply(df$ExterQual, 
           function(x) kv_ext_qual[kv_ext_qual$key == x, ]$value)
  df$ExterCond <- 
    sapply(df$ExterCond, 
           function(x) kv_ext_cond[kv_ext_cond$key == x, ]$value)
  df$BsmtQual <- 
    sapply(df$BsmtQual, 
           function(x) kv_bsmt_qual[kv_bsmt_qual$key == x, ]$value)
  df$BsmtCond <- 
    sapply(df$BsmtCond, 
           function(x) kv_bsmt_cond[kv_bsmt_cond$key == x, ]$value)
  df$BsmtExposure <- 
    sapply(df$BsmtExposure, 
           function(x) kv_bsmt_exp[kv_bsmt_exp$key == x, ]$value)
  df$BsmtFinType1 <- 
    ifelse(df$BsmtFinType1 == "GLQ", 2, 
           ifelse(df$BsmtFinType1 == "None", 0, 1))
  df$HeatingQC <- 
    sapply(df$HeatingQC, 
           function(x) kv_heat_qc[kv_heat_qc$key == x, ]$value)
  df$CentralAir <- ifelse(df$CentralAir == "Y", 1, 0)
  df$Electrical <- 
    sapply(df$Electrical,
           function(x) kv_electrical[kv_electrical$key == x, ]$value)
  df$KitchenQual <-
    sapply(df$KitchenQual,
           function(x) kv_kitchen[kv_kitchen$key == x, ]$value)
  df$FireplaceQu <-
    sapply(df$FireplaceQu,
           function(x) kv_fireplace_q[kv_fireplace_q$key == x, ]$value)
  df$GarageType <- 
    ifelse(df$GarageType %in% 
             c("2Types", "Attchd", "Basment", "BuiltIn"), 1, 0)
  df$GarageFinish <-
    sapply(df$GarageFinish,
           function(x) kv_garage_fin[kv_garage_fin$key == x, ]$value)
  df$GarageQual <- 
    ifelse(df$GarageQual %in% c("Ex", "Gd", "TA"), 1, 0)
  df$GarageCond <- 
    ifelse(df$GarageCond %in% c("Ex", "Gd", "TA"), 1, 0)
  df$PavedDrive <-
    sapply(df$PavedDrive,
           function(x) kv_paved_drive[kv_paved_drive$key == x, ]$value)
  df$PoolQC <- ifelse(df$PoolQC == "Ex", 1, 0)
  df$MoSold <- sapply(df$MoSold, function(x) month.name[x])
  i <- sapply(df, is.character)
  df[i] <- lapply(df[i], as.factor)
  
  # binary coding
  df$MasVnrArea <- ifelse(df$MasVnrArea > 0, 1, 0)
  df$MiscVal <- ifelse(df$MiscVal > 0, 1, 0)
  df$X3SsnPorch <- ifelse(df$X3SsnPorch > 0, 1, 0)
  df$ScreenPorch <- ifelse(df$ScreenPorch > 0, 1, 0)
  df$LowQualFinSF <- ifelse(df$LowQualFinSF > 0, 0, 1)
  
  ## log transform
  df$LotArea <- log(df$LotArea)
  df$GrLivArea <- log(df$GrLivArea)
  return(df)
}

drop_outliers <- function(dataset) {
  df <- dataset
  df <- df[df$BsmtFinSF1 < 5000, ]
  df <- df[df$X1stFlrSF < 4000, ]
  return(df)
}
```

```{r dummy-coding}
library(caret)

train <- replace_missing(train)
test <- replace_missing(test)

train_facts <- sapply(train[colnames(train[sapply(train, is.factor)])], function(x) sort(unique(x[!is.na(x)])))
test_facts <- sapply(test[colnames(train[sapply(train, is.factor)])], function(x) sort(unique(x[!is.na(x)])))

for (i in 1:length(test_facts)) {
  if (length(setdiff(test_facts[[i]], train_facts[[i]])) > 0) {
    print(names(test_facts)[i])
  }
}

unique(train$MSSubClass)
unique(test$MSSubClass)

train <- recode(train)
test <- recode(test)

train <- drop_outliers(train)

colSums(sapply(train, is.na))[colSums(sapply(train, is.na)) > 0]
colSums(sapply(test, is.na))[colSums(sapply(test, is.na)) > 0]

## dummy code categorical variables in `train` and `test` datasets 
dummies <- dummyVars("~ .", data = rbind(train[, -ncol(train)], test))
SalePrice <- data.frame(Id = train$Id, SalePrice = train$SalePrice)
train <- as.data.frame(predict(dummies, newdata = train))
train <- merge(train, SalePrice, by = "Id")
test <- as.data.frame(predict(dummies, newdata = test))

(nzv <- nearZeroVar(train, saveMetrics = TRUE))
nzv_cols <- row.names(nzv[!grepl("Neighborhood", row.names(nzv)) & nzv$nzv, ])
if(length(nzv_cols) > 0) {
  train <- train[, -which(names(train) %in% nzv_cols)]
}

## identify and remove highly correlated predictors from training set
cor_preds <- cor(train[, -which(names(train) == "SalePrice")])
high_cor <- findCorrelation(cor_preds, cutoff = 0.80)
which(colnames(train) %in% 
        c("GrLivArea", "TotalBsmtSF", 
          "GarageCars", "FireplaceQu"))
high_cor <- high_cor[!high_cor %in% c(80, 86, 96, 100)]
train <- train[, -high_cor]
```

#### Partition labeled training data

```{r split-training}
## partition training set for model testing on known sale prices
split <- createDataPartition(train$SalePrice, p = 0.75, list = FALSE)

training <- train[split, ]
testing <- train[-split, ]
```

#### Lookup regression models in _caret_

```{r lookup-regression-algos}
mods <- modelLookup()
mods <- mods[mods$forReg == TRUE, ]
```

#### Use 10-fold repeated cross validation for fitting

```{r fit-control}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5)
```

#### Train models

```{r train-models, warning = FALSE}
library(randomForest)
library(xgboost)
library(elasticnet)
library(glmnet)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

## model training and evaluation on partitioned `train` data
err <- data.frame(model = character(0), rmse = numeric(0),
                  stringsAsFactors = FALSE)
# set.seed(2017)
lm_fit <- train(log(SalePrice) ~ . - Id, data = training,
                method = "lm",
                preProc = c("center", "scale"),
                trControl = fitControl)
lm_fit
testing$SalePredict1 <- exp(predict(lm_fit, testing, na.action = na.pass))
RMSE(log(testing$SalePredict1), log(testing$SalePrice))
sqrt(sum((log(testing$SalePredict1) - log(testing$SalePrice))^2) /
       nrow(testing))
err[nrow(err) + 1, ] <-
  c("lm",
    RMSE(log(testing$SalePredict1),
         log(testing$SalePrice))
  )
summary(lm_fit)
# set.seed(2017)
# rf_fit <- train(log(SalePrice) ~ . - Id, data = training,
#                 method = "rf",
#                 preProc = c("center", "scale"),
#                 trControl = fitControl)
# rf_fit
# testing$SalePredict2 <- exp(predict(rf_fit, testing, na.action = na.pass))
# RMSE(log(testing$SalePredict2), log(testing$SalePrice))
# sqrt(sum((log(testing$SalePredict2) - log(testing$SalePrice))^2) /
#        nrow(testing))
# 
# err[nrow(err) + 1, ] <-
#   c("rf",
#     RMSE(log(testing$SalePredict2),
#          log(testing$SalePrice))
#   )
# set.seed(2017)
# xgbLin_fit <- train(log(SalePrice) ~ . - Id, data = training,
#                     method = "xgbLinear",
#                     preProc = c("center", "scale"),
#                     trControl = fitControl)
# xgbLin_fit
# testing$SalePredict3 <- exp(predict(xgbLin_fit, testing, na.action = na.pass))
# RMSE(log(testing$SalePredict3), log(testing$SalePrice))
# sqrt(sum((log(testing$SalePredict3) - log(testing$SalePrice))^2) /
#        nrow(testing))
# 
# err[nrow(err) + 1, ] <- 
#   c("xgbLin", 
#     RMSE(log(testing$SalePredict3),
#          log(testing$SalePrice))
#   )
# set.seed(2017)
# xgbTree_fit <- train(log(SalePrice) ~ . - Id, data = training,
#                      method = "xgbTree",
#                      preProc = c("center", "scale"),
#                      trControl = fitControl)
# xgbTree_fit
# testing$SalePredict4 <- exp(predict(xgbTree_fit, testing, na.action = na.pass))
# RMSE(log(testing$SalePredict4), log(testing$SalePrice))
# sqrt(sum((log(testing$SalePredict4) - log(testing$SalePrice))^2) /
#        nrow(testing))
# 
# err[nrow(err) + 1, ] <- 
#   c("xgbTree", 
#     RMSE(log(testing$SalePredict4),
#          log(testing$SalePrice))
#   )
# set.seed(2017)
# ridge_fit <- train(log(SalePrice) ~ . - Id, data = training,
#                  method = "ridge",
#                  preProc = c("center", "scale"),
#                  trControl = fitControl)
# ridge_fit
# testing$SalePredict5 <- exp(predict(ridge_fit, testing, na.action = na.pass))
# RMSE(log(testing$SalePredict5), log(testing$SalePrice))
# sqrt(sum((log(testing$SalePredict5) - log(testing$SalePrice))^2) /
#        nrow(testing))
# 
# err[nrow(err) + 1, ] <-
#   c("ridge",
#     RMSE(log(testing$SalePredict5),
#          log(testing$SalePrice))
#   )
# set.seed(2017)
# glmnet_fit <- train(log(SalePrice) ~ . - Id, data = training,
#                  method = "glmnet",
#                  preProc = c("center", "scale"),
#                  trControl = fitControl)
# glmnet_fit
# testing$SalePredict6 <- exp(predict(glmnet_fit, testing, na.action = na.pass))
# RMSE(log(testing$SalePredict6), log(testing$SalePrice))
# sqrt(sum((log(testing$SalePredict6) - log(testing$SalePrice))^2) /
#        nrow(testing))
# 
# err[nrow(err) + 1, ] <-
#   c("glmnet",
#     RMSE(log(testing$SalePredict6),
#          log(testing$SalePrice))
#   )

err[order(err$rmse), ]

## examine correlations across model predictions
# cor(testing[, (ncol(testing)-6):ncol(testing)])

## re-train models on entire training dataset
# set.seed(2017)
lm_full <- train(log(SalePrice) ~ . - Id, data = train,
                     method = "lm",
                     preProc = c("center", "scale"),
                     trControl = fitControl)
lm_full
# set.seed(2017)
# rf_full <- train(log(SalePrice) ~ . - Id, data = train,
#                      method = "rf",
#                      preProc = c("center", "scale"),
#                      trControl = fitControl)
# rf_full
# set.seed(2017)
# xgbLin_full <- train(log(SalePrice) ~ . - Id, data = train,
#                      method = "xgbLinear",
#                      preProc = c("center", "scale"),
#                      trControl = fitControl)
# xgbLin_full
# set.seed(2017)
# xgbTree_full <- train(log(SalePrice) ~ . - Id, data = train,
#                      method = "xgbTree",
#                      preProc = c("center", "scale"),
#                      trControl = fitControl)
# xgbTree_full
# set.seed(2017)
# ridge_full <- train(log(SalePrice) ~ . - Id, data = train,
#                      method = "ridge",
#                      preProc = c("center", "scale"),
#                      trControl = fitControl)
# ridge_full
# set.seed(2017)
# glmnet_full <- train(log(SalePrice) ~ . - Id, data = train,
#                      method = "glmnet",
#                      preProc = c("center", "scale"),
#                      trControl = fitControl)
# glmnet_full

stopCluster(cl)
```

#### Make predictions & output to CSV

```{r make-predictions}
## linear model prediction
test$SalePrice <- exp(predict(lm_full, test, na.action = na.pass))
test <- test[, which(names(test) %in% names(train))]

## xgbTree prediction
# test$SalePrice <- exp(predict(xgbTree_full, test, na.action = na.pass))
# test <- test[, which(names(test) %in% names(train))]

## combine model predictions by finding mean prediction for each property
# test$SalePrice <- exp(
#   rowMeans(data.frame(
#     predict(lm_full, test, na.action = na.pass),
#     predict(rf_full, test, na.action = na.pass),
#     predict(xgbLin_full, test, na.action = na.pass),
#     predict(xgbTree_full, test, na.action = na.pass),
#     predict(ridge_full, test, na.action = na.pass),
#     predict(glmnet_full, test, na.action = na.pass)),
#     na.rm = TRUE)
# )

predictions <- data.frame(Id = test$Id, SalePrice = test$SalePrice)
head(predictions)
predictions[is.na(predictions$SalePrice), ]

## save output, change filename as needed
# write.csv(predictions, file = "Submission_052317_lin2.csv", quote = FALSE, row.names = FALSE)
```

My best public root mean squared error (RMSE) score in Kaggle's House Prices: Advanced Regression Techniques competition was 0.12179 (user name: janderman, display name: Judd Anderman), which was the result of my most recent modeling attempt following a few rounds of iteration, error checking, and refinement.  In this case, I used only used my fitted linear model to predict property `SalePrice` in the unlabeled test dataset.  From my perspective, the relative success of this last submission was a result of missing data imputation - in most cases missing data points were in fact meaningful and so were fairly easy to impute - and recoding of the predictors and target variables as seemed appropriate in each case, whether that involved performing log transformations, binary coding, or casting categorical variables as numeric ones.  

*Addendum: I was able to achieve a slightly lower RMSE of 0.12033 on the public leaderboard data by averaging the output of several trained models applied to the `test` dataset, including the linear model I had used previously.  This latter approach was significantly more computationally intensive and time-consuming for what appears to be a relatively modest gain in predictive performance.  The relevant code is contained in the last couple of code chunks above but commented out, however, it can be found in a separate R markdown file.  While I found it productive to partition the training data so that I could evaluate and compare the performance of different models against known sale prices, I did find that retraining my chosen model(s) on the full training dataset produced improved results.  Still, my largest gains in RMSE, the competition's evaluation metric, occured early on after more careful examination and deliberate processing and transformation of the supplied training and testing data.
